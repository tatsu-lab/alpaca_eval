{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb4f80ae-d4bf-48f0-802b-a932de6c9759",
   "metadata": {},
   "source": [
    "# Length correction of AlpacaEval\n",
    "\n",
    "This notebook is about trying to correct for the length bias in AlpacaEval. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77e923ce-1f87-4c6c-87f3-70c0bca767b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanndubois/Desktop/GitHub/alpaca_eval\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406fc95c-073a-4952-818b-712ceac94c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3ad57-504d-4460-a489-113c0082f3cf",
   "metadata": {},
   "source": [
    "## Desiderata?\n",
    "\n",
    "Here are the desired properties that we would like for our length-corrected win-rate.\n",
    "\n",
    "Here are the things that I think we should be considering when deciding\n",
    "1. **(D1) Have high correlation with humans** ultimately the most important property for AlpacaEval is that it's highly correlated with humans. Arguably this is a property that we already have, so we should just make sure that the correlation of the length-corrected win-rate is at least as high, and ideally higher than the current correlation. Note that we have high-correlations with human despite the length bias because (1) humans are also length-biased; and (2) we have validated our auto-annotators compared to humans only in standard/realistic scenario rather than in extreme/\"worst-case\" ones. This brings us to D2.\n",
    "2. **(D2) Low length gameability** one major issue with a bias metric is that adversarial players may take advantage of it, i.e., game the metric. This might not be seen when looking at standard correlation with humans because we rarely test gamed systems. Given that correlation with humans is already very high (D1), we see (D2) as the major new property that we are trying to achieve. We would thus ideally want gamed systems (e.g. by changing the system prompt to be verbose) to not change too much. Note that humans might not actually have such property, so we are asking more from our auto-annotators than humans. \n",
    "This is nevertheless very important for a leaderboard like AlpacaEval, because people usually think about it as an evalaution of a model rather than a system (model+prompt). We thus want to avoid having large prompt gameability so that it behaves closer to what people have in mind. \n",
    "3. **(D3) Similar length bias as humans**: human are typically length biased, the goal is thus not to be length-invariant but to match human biases. \n",
    "4. **(D4) Simplicity**: the model length correction should be simple enough for people to understand what is happening and what are limitations of the length corrections. Let's not hide the limitations (and potential gameability) behind complexity.\n",
    "5. **(D5) Similar interpretation as win-rate**: given that we'll use for the default AE metric, we would like the interpretation of the result to be as similar as  possible as the non-bias corrected ones given that people are used to those. I.e. preference for having 50% for the baseline, the range being between 0-100, ...\n",
    "6. **(D6) General procedure**: although length-bias is the biggest issue, AlpacaEval has other biases (e.g. bias towards lists, bias towards outputs similar to those from the autoannotator, ...). We thus like a general procedure to deal with biases so that we can reuse it when needed, and others can use simialr procedures for their benchmark.\n",
    "7. **(D7) Robustness**: we want to avoid as much as possible having settings in which the length-corrected metric can be gamed in a different way. E.g. by generating extremely short answers.\n",
    "8. **(D8) Independence between models**: as much as possible we would like our correction to not depend on the outputs / values / ranking of other models. In particular, we want the length-corrected metric to not have to be updated when future models (e.g. that are very long) are developed.\n",
    "\n",
    "In addition to the aforementioned desired properties, we have a hard constraint that there should be **no reevaluation needed**. I.e., the new metric should be a function of what we currently have to ensure that we can port the leaderboard.\n",
    "\n",
    "Now let's check how to decide whether a method is better or worse, i.e., what metrics to consider for the rest of the notebook.\n",
    "\n",
    "## Metrics?\n",
    "\n",
    "Stratified arena correlation per length. Per output.\n",
    "\n",
    "Here are the metrics I think we should be considering, one for each of the three desiderata.\n",
    "\n",
    "- **(D1) High Spearman correlation with Chatbot Arena Leaderboard.** This is the most important, and Chatbot arena as source of human truth given that it's the only large scale and in-the-wild chat leaderboard based on human data. The question then becomes which correlations. Given that ELO and win rate are very different metrics (e.g. one is bounded and not the other) there's no reason to hope that linear correlation holds, so we shouldn't use Pearson. More importantly, we only care about relative values with a benchmark like AlpacaEval (the usecase being model selection) so non-parametric correlation is better suited. Note that the choice of Spearman vs Kendall doesn't matter much here, we will use Spearman because it's on \"the same scale\" as Pearson correlation which people are more used to (e.g. Kendall is much smaller which always surprises people because they are not used to interpreting it). Also note that when using non-parametric correlations it's extremely important to fix the number of points you are comparing, otherwise more points always looks better. \n",
    "\n",
    "  \n",
    "- **(D2) Small relative variance when prompting for verbosity/conciseness.** One heuristic to avoid gameability is to ensure that the win-rate for a model is close to the win-rate of the same model prompted for verbosity. To test that we will use a few models and prompt them (1) normally, (2) to \"give as much detail as possible\", (3) to \"be as concise as possible while still providing all the necessary information to answer the question\". Then we will look at the variance between those three, which ideally shouldn't be too large. We will use GPT4-turbo, Claude 2, GPT3-turbo, GPT4, Mixtral. In particular, we will consider the relative standard deviation. Standard deviation, simply because it's easier to interpret. Relative in the sense that we (1) normalize by the non-gamed win rate, and (2) we normalize by the standard deviation of different models, i.e., squishing the range of the metric shouldn't solve the gameability issue. Note that ideally, we would actually estimate the gameability of humans and be able to compare to that. Unfortunately LMSys doesn't evaluate the same model with different prompts. @todo: ask them.\n",
    "\n",
    "\n",
    "- **(D3) Same Spearman correlation AE-Length as Arena-Length.** To track the length bias of humans we can see how correlated the Arena leaderboard is with a leaderboard that simply ranks outputs by their lengths. Again we use Spearman correlations.\n",
    "\n",
    "\n",
    "- **(D1) Have high correlation with humans** \n",
    "- **(D2) Low length gameability** \n",
    "- **(D3) Similar length bias as humans**\n",
    "- **(D4) Simplicity**\n",
    "- **(D5) Similar interpretation as win-rate**\n",
    "- **(D6) General procedure**\n",
    "- **(D7) Robustness**\n",
    "- **(D8) Independence between models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f21ede-eef2-4cd1-a5cd-c0e76be7d143",
   "metadata": {},
   "source": [
    "## Setting up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc4760f6-fd06-4d2c-bced-f0bc0f63c4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from alpaca_eval import utils, metrics, annotators, constants, analyze, plotting, main\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from patsy import dmatrix\n",
    "from sklearn.linear_model import LogisticRegressionCV, LinearRegression, RidgeCV, LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import sklearn\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n",
    "from patsy import build_design_matrices\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "\n",
    "BASELINE = \"gpt4_1106_preview\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953043d0-d527-4717-bd89-0971b1d39211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are comparing to 33 Arena models\n"
     ]
    }
   ],
   "source": [
    "lb = pd.read_csv(\"src/alpaca_eval/leaderboards/data_AlpacaEval_2/weighted_alpaca_eval_gpt4_turbo_leaderboard.csv\", index_col=0)\n",
    "\n",
    "def make_lb_arena(lb):\n",
    "    lb_arena = lb.loc[list(constants.CHATBOT_ARENA_LEADERBOARD.keys()),:]\n",
    "    lb_arena[\"ELO\"] = constants.CHATBOT_ARENA_LEADERBOARD.values()\n",
    "    return lb_arena\n",
    "\n",
    "lb_arena = make_lb_arena(lb)\n",
    "print(f\"We are comparing to {len(lb_arena)} Arena models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb37f4f-7636-4dd2-9230-9a3b39cf2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "game_process_v = lambda s : s.replace(\"_verbose\",\"\")\n",
    "game_process_c = lambda s : s.replace(\"_concise\",\"\")\n",
    "gamed_models = [i for i in lb.index\n",
    "               if (i + \"_verbose\") in lb.index and (i + \"_concise\") in lb.index]\n",
    "diff_models = [i for i in lb.index\n",
    "               if \"_verbose\" not in i and i + \"_concise\" not in i]\n",
    "lb[\"gamed_verbose_only\"] = [game_process_v(i) if game_process_v(i) in gamed_models else None for i in lb.index]\n",
    "lb[\"gamed_concise_only\"] = [game_process_c(i) if game_process_c(i) in gamed_models else None for i in lb.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e353f-6edc-480c-9be1-d8c07ce3c110",
   "metadata": {},
   "source": [
    "Here are the models that we gamed (i.e. prompted for verbosity/conciseness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b341e73-e391-416c-96af-7d6f50612c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt4_1106_preview',\n",
       " 'Mixtral-8x7B-Instruct-v0.1',\n",
       " 'gpt4_0613',\n",
       " 'claude-2.1',\n",
       " 'gpt-3.5-turbo-1106',\n",
       " 'alpaca-7b']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamed_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e99ccfe5-4e0f-406a-9d2d-62599c9e3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report(lb, metric, is_detailed=False, n_toshow=10, is_return_metrics=False):\n",
    "    lb_arena = make_lb_arena(lb)\n",
    "\n",
    "    if not is_return_metrics:\n",
    "        print(f\"# Report for **{metric}**\")\n",
    "\n",
    "        print()\n",
    "        print(\"## Gameability (lower is better)\")\n",
    "\n",
    "    df_gamed_v = lb.groupby(\"gamed_verbose_only\")[[\"avg_length\", metric]].agg([\"mean\",\"std\"]) \n",
    "    df_gamed_c = lb.groupby(\"gamed_concise_only\")[[\"avg_length\", metric]].agg([\"mean\",\"std\"]) \n",
    "    # relative in the sense that models with larger metric shouldn't be considered as having larger vairance\n",
    "    df_gamed_v[(metric, 'rel_std')] = df_gamed_v[metric][\"std\"] /  df_gamed_v[metric][\"mean\"]\n",
    "    df_gamed_c[(metric, 'rel_std')] = df_gamed_c[metric][\"std\"] /  df_gamed_c[metric][\"mean\"] \n",
    "    # renormalize to avoid removing gameability by shrinking the scale of the metric\n",
    "    winrate_std_across_models = lb[lb.index.isin(diff_models)][\"win_rate\"].std()\n",
    "    metric_std_across_models = lb[lb.index.isin(diff_models)][metric].std()\n",
    "    metric_weight = winrate_std_across_models / metric_std_across_models \n",
    "    \n",
    "    if is_detailed:\n",
    "        print(f\"metric_weight: {metric_weight:.3f}\")\n",
    "        display(df_gamed_v)\n",
    "        display(df_gamed_c)\n",
    "\n",
    "    verbosity_gameability = df_gamed_v[metric]['rel_std'].mean() * metric_weight * 100\n",
    "    conciseness_gameability = df_gamed_c[metric]['rel_std'].mean() * metric_weight * 100\n",
    "    \n",
    "    if not is_return_metrics:\n",
    "        print(f\"Verbosity gameability (relative std metric): {verbosity_gameability:.1f}%\")\n",
    "        print(f\"Conciseness gameability (relative std metric): {conciseness_gameability:.1f}%\")\n",
    "\n",
    "        print()\n",
    "        print(\"## Correlation with Arena (higher is better)\")\n",
    "\n",
    "    corr_arena = print_correlations(lb_arena[metric], lb_arena[\"ELO\"], is_return_metrics=is_return_metrics)\n",
    "\n",
    "    if not is_return_metrics:\n",
    "        print()\n",
    "        arena_corr = print_correlations(lb_arena[\"ELO\"],\n",
    "                                       lb_arena[\"avg_length\"],\n",
    "                                        \"Arena vs Length\", \n",
    "                                        is_return_metrics=True)\n",
    "        print(f\"## Correlation with length (closer to spearman={arena_corr['spearman']:.2f}, kendall={arena_corr['kendall']:.2f} is better)\")\n",
    "    \n",
    "    corr_len = print_correlations(lb_arena[metric], lb_arena[\"avg_length\"], is_return_metrics=is_return_metrics)\n",
    "\n",
    "    if not is_return_metrics:\n",
    "        print()\n",
    "        print(f\"## Top {n_toshow} models\")\n",
    "    \n",
    "        display(lb[metric].sort_values(ascending=False)[:n_toshow])\n",
    "    \n",
    "        print()\n",
    "        print(f\"## Bottom {n_toshow} models\")\n",
    "    \n",
    "        display(lb[metric].sort_values(ascending=False)[-n_toshow:])\n",
    "\n",
    "    if is_return_metrics:\n",
    "        return dict(verbosity_gameability=verbosity_gameability, \n",
    "                    conciseness_gameability=conciseness_gameability,\n",
    "                   corr_arena=corr_arena[\"spearman\"],\n",
    "                    corr_len=corr_len[\"spearman\"])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd617806-9a08-45f4-9dd6-6ef64703e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_correlations(arr1, arr2, txt=\"\", is_return_metrics=False):\n",
    "    if isinstance(arr1, pd.DataFrame):\n",
    "        arr1 = list(arr1.index)\n",
    "    if isinstance(arr2, pd.DataFrame):\n",
    "        arr2 = list(arr2.index)\n",
    "    s = scipy.stats.spearmanr(arr1, arr2).statistic\n",
    "    t = scipy.stats.kendalltau(arr1, arr2).statistic\n",
    "    \n",
    "    if is_return_metrics:\n",
    "        return dict(spearman=s, kendall=t)\n",
    "    else:\n",
    "        if txt != \"\":\n",
    "            txt = txt + \"\\n\"\n",
    "        print(f\"{txt}Spearman Corr: {s:.3f}\\nKendall Corr: {t:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684ddf6f-d338-4703-a89a-a24686922f03",
   "metadata": {},
   "source": [
    "Here are the correlations between Arena and length that we should try to be close to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2dee0d-b579-4b91-a55c-503c6e1192e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arena vs Length\n",
      "Spearman Corr: 0.236\n",
      "Kendall Corr: 0.163\n"
     ]
    }
   ],
   "source": [
    "print_correlations(lb_arena[\"ELO\"],\n",
    "                   lb_arena[\"avg_length\"],\n",
    "                  \"Arena vs Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821ff4c-1e7b-4970-ac5d-028b5945d5c0",
   "metadata": {},
   "source": [
    "## Raw win rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c03e2-ef7c-4588-baef-028c8b96280e",
   "metadata": {},
   "source": [
    "- **What**: compute the expected number of times that the model is better than the baseline. This is the default in AE.\n",
    "- **Benefits**: simple\n",
    "- **Downside**: length bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f772b9-8ee7-470c-a9ac-4027fbf978d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report for **win_rate**\n",
      "\n",
      "## Gameability (lower is better)\n",
      "Verbosity gameability (relative std metric): 21.3%\n",
      "Conciseness gameability (relative std metric): 29.8%\n",
      "\n",
      "## Correlation with Arena (higher is better)\n",
      "Spearman Corr: 0.921\n",
      "Kendall Corr: 0.788\n",
      "\n",
      "## Correlation with length (closer to spearman=0.24, kendall=0.16 is better)\n",
      "Spearman Corr: 0.467\n",
      "Kendall Corr: 0.324\n",
      "\n",
      "## Top 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpt4_1106_preview_verbose                64.303601\n",
       "gpt4_1106_preview                        50.000000\n",
       "Snorkel-Mistral-PairRM-DPO-best-of-16    34.860133\n",
       "pairrm-Yi-34B-Chat                       31.241283\n",
       "Snorkel-Mistral-PairRM-DPO               30.220053\n",
       "Yi-34B-Chat                              29.659947\n",
       "Qwen1.5-72B-Chat                         26.498283\n",
       "Mixtral-8x7B-Instruct-v0.1_verbose       24.614063\n",
       "claude-2.1_verbose                       24.354071\n",
       "gpt4                                     23.576789\n",
       "Name: win_rate, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Bottom 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "guanaco-7b              2.880002\n",
       "chatglm2-6b             2.762185\n",
       "text_davinci_001        2.757166\n",
       "alpaca-7b               2.591451\n",
       "pythia-12b-mix-sft      2.578090\n",
       "phi-2                   2.344452\n",
       "falcon-7b-instruct      2.146618\n",
       "baichuan-13b-chat       1.992146\n",
       "alpaca-7b_concise       1.988703\n",
       "oasst-sft-pythia-12b    1.790114\n",
       "Name: win_rate, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(lb, \"win_rate\", is_detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f98342d-ccc8-47b5-953f-6231954dc601",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- gameability due to asking for details is high\n",
    "- correlation with Arena is relatively high\n",
    "- correlation with length is >2x than Arena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60c2a32-f55b-4ec9-aa04-7273e2fa79c1",
   "metadata": {},
   "source": [
    "# Proposed metrics to consider\n",
    "\n",
    "First, let's compute some values---based on annotations---that will be useful for all the proposed metrics. Note throughout the rest that 1 -> baseline, and 2 -> model being evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89c9ffc7-430a-4e78-9f95-aa7e4f03593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(lb):\n",
    "    \"\"\"Load annotations from models in lb and add some statistics that may be useful.\"\"\"\n",
    "    annotations = {}\n",
    "        \n",
    "    for i in lb.index:\n",
    "        # load actual annotations to see if it was longer or not \n",
    "        df_annotations = pd.read_json(f\"results/{i}/weighted_alpaca_eval_gpt4_turbo/annotations.json\")\n",
    "        df_annotations[\"len_1\"] = df_annotations[\"output_1\"].str.len()  \n",
    "        df_annotations[\"len_2\"] = df_annotations[\"output_2\"].str.len()  \n",
    "        df_annotations[\"is_longer2\"] = df_annotations[\"len_1\"] < df_annotations[\"len_2\"]     \n",
    "        df_annotations[\"is_longer1\"] = df_annotations[\"len_2\"] < df_annotations[\"len_1\"] \n",
    "        df_annotations[\"is_same_length\"] = df_annotations[\"len_2\"] == df_annotations[\"len_1\"] \n",
    "        df_annotations[\"model\"] = i\n",
    "        annotations[i] = df_annotations.reset_index().drop(columns=[\"raw_completion\", \"output_2\", \"output_1\", \"instruction\"]) # drop all the long stuff that is not needed\n",
    "\n",
    "    df_annotations = pd.concat(annotations, ignore_index=True).query(\"preference >= 0\")\n",
    "    df_annotations[\"preference\"] = (df_annotations[\"preference\"].astype(float).replace({0.0: 1.5}) - 1) # easier to work with\n",
    "    return df_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7bb7a41-6630-4c24-aad9-465ef08acbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics_vs_param(get_new_win_rate, parameters, metric_name, **kwargs):\n",
    "    verbosity_gameability = []\n",
    "    conciseness_gameability = []\n",
    "    corr_arena = []\n",
    "    corr_len = []\n",
    "    \n",
    "    for parameter in parameters:\n",
    "        lb_longer[metric_name] = get_new_win_rate(lb_longer, parameter, **kwargs)\n",
    "        metrics = report(lb_longer, metric_name, is_detailed=False, is_return_metrics=True)\n",
    "        verbosity_gameability.append(metrics[\"verbosity_gameability\"])\n",
    "        conciseness_gameability.append(metrics[\"conciseness_gameability\"])\n",
    "        corr_len.append(metrics[\"corr_len\"])\n",
    "        corr_arena.append(metrics[\"corr_arena\"])\n",
    "\n",
    "        if metrics[\"corr_arena\"] >= pd.Series(corr_arena).max():\n",
    "            lb_longer[metric_name+\"_best\"] = lb_longer[metric_name]\n",
    "\n",
    "    metrics_w = pd.DataFrame(dict(parameter=parameters, verbosity_gameability=verbosity_gameability, conciseness_gameability=conciseness_gameability, corr_arena=corr_arena, corr_len=corr_len))\n",
    "    to_plot = metrics_w.melt(value_name=\"value\", var_name=\"metric\", value_vars=metrics.keys(), id_vars=\"parameter\")\n",
    "    g= sns.relplot(to_plot, x='parameter', \n",
    "                       y='value', col='metric', kind=\"line\", \n",
    "                        facet_kws=dict(sharey=False))\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21f7f9ac-59bd-46ef-8f46-e4c1192dd925",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df_annotations = load_annotations(lb)\n",
    "all_df_annotations = all_df_annotations.query(\"len_2 != 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e3dc6-811b-4ee8-9dd7-6bb50908ea06",
   "metadata": {},
   "source": [
    "## Balanced win rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e26e1-ae68-4cc4-b188-987f2de43263",
   "metadata": {},
   "source": [
    "Background: oen common way to control for a covariate that you don't want to take into account is by stratification. Here we will stratify the data by \"longer than baseline\" and \"shorter than baseline\".\n",
    "\n",
    "- **What**: compute the expected number of times that the model is better than the baseline when the outputs of the model are (1) longer, and (2) shorter than the baseline. Then take the average between both settings.\n",
    "- **Benefits**:\n",
    "    - **(D1-D3) metrics**: much better in all metrics than raw win-rate, although seems to overcorrect length bias.\n",
    "    - **(D4) simplicity**: simple to understand\n",
    "    - **(D5) interpretability**: simple to interpret & similar interpretation as win-rate\n",
    "    - **(D8) independence**: can apply the correction independently of each model.\n",
    "- **Downside**:\n",
    "    - there might be a cofounder between the complexity of the task and which outputs are longer.\n",
    "    - **(D6) generality**: although the  procedure. Hard to apply for other biases, given that it would split the space exponentially.\n",
    "    - **(D7) robustness**: not robust for cases where there are very few answer that are either short or longer than the baseline.  High variance / gameable if models have only a few outputs in one of the two categories. E.g. everything is shorter than baseline => should not allow evla of such model.\n",
    " \n",
    "Overall I like this simple (to compute & to interpret) controlled-length. I think it's better in most respect than the raw win rate. My major worry is that it makes the metric really bad in the case of models that are nearly always shorter or longer than the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "992e25da-c3d0-4fcd-ba56-07f2d2e3cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_metrics_for_longer_balancing(lb, all_df_annotations):\n",
    "    \"\"\"Computes the mean and variance fo win rate grouped by whether the model is longer/shorter than the baseline\"\"\"\n",
    "    lb = lb.copy()\n",
    "    keys = [\"mean_1longer\", \"mean_2longer\", \"var_1longer\", \"var_2longer\", \"count_2longer\", \"count_2longer\",\n",
    "           \n",
    "           ]\n",
    "    annotations = {}\n",
    "    for k in keys:\n",
    "        lb[k] = None\n",
    "        \n",
    "    for i in lb.index:\n",
    "        # load actual annotations to see if it was longer or not \n",
    "        df_annotations = all_df_annotations[all_df_annotations.model == i]\n",
    "\n",
    "        lb.loc[i,\"mean\"] = df_annotations[\"preference\"].mean()\n",
    "        groupby_islonger1 = df_annotations.groupby(\"is_longer1\")[\"preference\"].agg([\"mean\", \"var\"])\n",
    "        groupby_islonger2 = df_annotations.groupby(\"is_longer2\")[\"preference\"].agg([\"mean\", \"var\"])\n",
    "        is_same_length = df_annotations[\"is_same_length\"].sum()        \n",
    "\n",
    "        # uses islonger1/2 instead of true false to deal with same lengths\n",
    "        try:\n",
    "            lb.loc[i, \"mean_1longer\"] = groupby_islonger1.loc[True, \"mean\"]\n",
    "            lb.loc[i, \"mean_2longer\"] = groupby_islonger2.loc[True, \"mean\"]\n",
    "            lb.loc[i, \"var_1longer\"] = groupby_islonger1.loc[True, \"var\"]\n",
    "            lb.loc[i, \"var_2longer\"] = groupby_islonger2.loc[True, \"var\"]\n",
    "            \n",
    "        except: # case where all is shorter or longer or same\n",
    "            lb.loc[i, \"mean_1longer\"] = lb.loc[i, \"mean_2longer\"] = df_annotations[\"preference\"].mean()\n",
    "            lb.loc[i, \"var_1longer\"] = lb.loc[i, \"var_2longer\"] = df_annotations[\"preference\"].var()\n",
    "        \n",
    "        lb.loc[i, \"count_1longer\"] = df_annotations[\"is_longer1\"].sum() + is_same_length/2\n",
    "        lb.loc[i, \"count_2longer\"] = df_annotations[\"is_longer2\"].sum() + is_same_length/2\n",
    "            \n",
    "\n",
    "    return lb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a5190e3-bead-48e0-b8b0-ae53f0218459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report for **balanced_win_rate**\n",
      "\n",
      "## Gameability (lower is better)\n",
      "Verbosity gameability (relative std metric): 12.5%\n",
      "Conciseness gameability (relative std metric): 19.0%\n",
      "\n",
      "## Correlation with Arena (higher is better)\n",
      "Spearman Corr: 0.929\n",
      "Kendall Corr: 0.792\n",
      "\n",
      "## Correlation with length (closer to spearman=0.24, kendall=0.16 is better)\n",
      "Spearman Corr: 0.125\n",
      "Kendall Corr: 0.085\n",
      "\n",
      "## Top 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpt4_1106_preview_verbose                55.464179\n",
       "gpt4_1106_preview                             50.0\n",
       "gpt4_1106_preview_concise                42.476316\n",
       "Qwen1.5-72B-Chat                         37.072419\n",
       "gpt4                                     36.503486\n",
       "gpt4_0613_verbose                        32.752569\n",
       "Snorkel-Mistral-PairRM-DPO-best-of-16     31.50451\n",
       "claude                                   31.090175\n",
       "gpt4_0314                                30.972772\n",
       "mistral-medium                           30.845477\n",
       "Name: balanced_win_rate, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Bottom 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "llama-2-7b-chat-hf      6.711881\n",
       "alpaca-7b_concise       6.673438\n",
       "baize-v2-7b             5.322234\n",
       "guanaco-7b              4.644985\n",
       "chatglm2-6b             4.570916\n",
       "guanaco-13b             4.103434\n",
       "pythia-12b-mix-sft      3.344637\n",
       "phi-2                   3.105325\n",
       "baichuan-13b-chat       2.464749\n",
       "oasst-sft-pythia-12b    2.274166\n",
       "Name: balanced_win_rate, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lb_longer = add_metrics_for_longer_balancing(lb, all_df_annotations)\n",
    "lb_longer[\"balanced_win_rate\"] = ((lb_longer[\"mean_1longer\"] + lb_longer[\"mean_2longer\"])/2)*100\n",
    "report(lb_longer, \"balanced_win_rate\", is_detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7485108-8060-4ae9-8f20-ce79a4082f66",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- gameability reduced by ~1.5-2x\n",
    "- correlation with Arena increased slightly\n",
    "- correlation with length is very small.\n",
    "\n",
    "Overall, this seems like a net gain. Note that the correlation with the length leaderboard is much smaller than humans, suggesting that we might have overcorrected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f7d4e-e59c-4fb1-8e12-bc595d0c54e9",
   "metadata": {},
   "source": [
    "## Average length normalized win-rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d33b8-af2e-4dc8-bb07-cc8117c958a8",
   "metadata": {},
   "source": [
    "- **What**: Normalize the win-rate the bounded difference between the average baseline length and the average of the current model. To bound that difference, we squash it through a logistic function with a temperature that we hyperparameter tuned. Note taht we rescale everything so that the baseline stays at 50%. Also note that we could also rescale on a per sample basis rather than on the average. Both give similar results.\n",
    "- **Benefits**:\n",
    "    - **(D1-D3) metrics**: much better in all metrics than raw win-rate. **Metrics are better than balanced win-rate**.\n",
    "    - **(D4) simplicity**: simple to understand\n",
    "    - **(D8) independence**: can apply the correction independently of each model.\n",
    "- **Downside**:\n",
    "    - there might be a cofounder between the complexity of the task and which outputs are longer.\n",
    "    - **(D5) interpretability**: pretty hard to interpret, especially given that the mdoels are actually not compared on the same scale anymore (the unit is different), as not all models can achieve a win rate [0,100]. The range depends on the length of outputs. \n",
    "    - **(D6) generality**: not general\n",
    "    - **(D7) robustness**: Clearly wrong in the limits of long / short lengths. Really not robust because unclear functional form: will this hold for future models? will the temperature generalize? given that it was choosen emperically but across all models, it's unclear if there's something deep about the current functional form. \n",
    " \n",
    "Overall I'm honestly surprised at how good this metric performs. My main concerns are (1) that the functional form is clearly chosen on the current models and there's little reason to believe that it will generalize, and (2) the lack of interpretability given that the scaleof the metric is now length dependent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "059e43b2-866c-43df-b232-4d17d4f60286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25eb7dd6-4d01-4c57-b6f4-959fe3b90b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report for **avg_sigmoid_length_corrected_win_rate**\n",
      "\n",
      "## Gameability (lower is better)\n",
      "Verbosity gameability (relative std metric): 13.4%\n",
      "Conciseness gameability (relative std metric): 16.9%\n",
      "\n",
      "## Correlation with Arena (higher is better)\n",
      "Spearman Corr: 0.951\n",
      "Kendall Corr: 0.822\n",
      "\n",
      "## Correlation with length (closer to spearman=0.24, kendall=0.16 is better)\n",
      "Spearman Corr: 0.260\n",
      "Kendall Corr: 0.165\n",
      "\n",
      "## Top 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpt4_1106_preview            50.000000\n",
       "gpt4_1106_preview_verbose    42.502532\n",
       "gpt4_1106_preview_concise    39.481617\n",
       "Qwen1.5-72B-Chat             38.743595\n",
       "claude-2.1_verbose           38.028529\n",
       "gpt4                         37.584082\n",
       "gpt4_0613_verbose            35.315026\n",
       "gpt4_0314                    35.101286\n",
       "mistral-medium               32.778640\n",
       "claude-2                     30.132118\n",
       "Name: avg_sigmoid_length_corrected_win_rate, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Bottom 10 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "alpaca-7b               4.999603\n",
       "chatglm2-6b             4.890945\n",
       "pythia-12b-mix-sft      4.674240\n",
       "guanaco-7b              4.592913\n",
       "phi-2                   4.431538\n",
       "guanaco-13b             4.400390\n",
       "falcon-7b-instruct      4.115462\n",
       "alpaca-7b_concise       3.848456\n",
       "oasst-sft-pythia-12b    3.343088\n",
       "baichuan-13b-chat       2.612329\n",
       "Name: avg_sigmoid_length_corrected_win_rate, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temperature=500\n",
    "lb[\"avg_sigmoid_length_corrected_win_rate\"] = lb[\"win_rate\"] * sigmoid((lb.loc[BASELINE, \"avg_length\"] - lb[\"avg_length\"] ) / temperature) * 2\n",
    "report(lb, \"avg_sigmoid_length_corrected_win_rate\", is_detailed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff021b62-0fa4-41a4-8c04-bb92e209718d",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- gameability is low (similar to balanced win-rate)\n",
    "- length correlation is similar to Arena (better than balanced win-rate)\n",
    "- human correlation is extremely high (better than balanced win-rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac6673-796a-4761-b6a0-fa4320eb5d91",
   "metadata": {},
   "source": [
    "\n",
    "## Causal inference and mediated effect\n",
    "\n",
    "**Background**: one extreme intepretation of a length-corrected metric, is the answer to \"what would be the metrics if all models had the same length as the baseline\". This enters the relm of causal inference where we are asking ourselves \"what is the direct impact of the model generating the output\". In particular, we don't want to consider the impact through \"bias\" variable such as length. Such variables are called mediators. Note that we don't want to rerun anything so we have to use techniques from causal inference for observational data.\n",
    "\n",
    "Here's the graphical representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d7487b20-5282-4d0b-9291-8d41f9049694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (20230707.0739)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"648pt\" height=\"190pt\"\n",
       " viewBox=\"0.00 0.00 648.37 190.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 186)\">\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-186 644.37,-186 644.37,4 -4,4\"/>\n",
       "<!-- M -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>M</title>\n",
       "<ellipse fill=\"green\" stroke=\"black\" cx=\"230.75\" cy=\"-164\" rx=\"35.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.75\" y=\"-158.95\" font-family=\"Times,serif\" font-size=\"14.00\">Model</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"red\" stroke=\"black\" cx=\"68.75\" cy=\"-91\" rx=\"68.75\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"68.75\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\">Other Mediator</text>\n",
       "</g>\n",
       "<!-- M&#45;&gt;B -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>M&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M204.36,-151.43C179.66,-140.61 142.28,-124.22 113.01,-111.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"114.6,-107.83 104.03,-107.02 111.79,-114.24 114.6,-107.83\"/>\n",
       "</g>\n",
       "<!-- L -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>L</title>\n",
       "<ellipse fill=\"red\" stroke=\"black\" cx=\"230.75\" cy=\"-91\" rx=\"75.41\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"230.75\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\">Length of Output</text>\n",
       "</g>\n",
       "<!-- M&#45;&gt;L -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>M&#45;&gt;L</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M230.75,-145.81C230.75,-138.05 230.75,-128.68 230.75,-119.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"234.25,-120.03 230.75,-110.03 227.25,-120.03 234.25,-120.03\"/>\n",
       "</g>\n",
       "<!-- S -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>S</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"313.75\" cy=\"-18\" rx=\"43.67\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.75\" y=\"-12.95\" font-family=\"Times,serif\" font-size=\"14.00\">Win&#45;rate</text>\n",
       "</g>\n",
       "<!-- M&#45;&gt;S -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>M&#45;&gt;S</title>\n",
       "<path fill=\"none\" stroke=\"green\" d=\"M259.29,-153.09C278.75,-144.66 303.04,-130.46 314.75,-109 325.05,-90.15 324,-65.59 320.82,-46.89\"/>\n",
       "<polygon fill=\"green\" stroke=\"green\" points=\"324.05,-46.22 318.65,-37.11 317.19,-47.6 324.05,-46.22\"/>\n",
       "<text text-anchor=\"middle\" x=\"379.25\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"green\"> &#160;Desired interaction</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;S -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>B&#45;&gt;S</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M113.69,-76.98C157.59,-64.26 224.07,-44.99 268.19,-32.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.99,-35.33 277.62,-29.18 267.04,-28.6 268.99,-35.33\"/>\n",
       "</g>\n",
       "<!-- L&#45;&gt;S -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>L&#45;&gt;S</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M250.42,-73.17C261.38,-63.8 275.17,-52 287.09,-41.81\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"289.04,-43.89 294.36,-34.73 284.49,-38.57 289.04,-43.89\"/>\n",
       "</g>\n",
       "<!-- I -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>I</title>\n",
       "<ellipse fill=\"lightgray\" stroke=\"black\" cx=\"555.75\" cy=\"-91\" rx=\"84.62\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"555.75\" y=\"-85.95\" font-family=\"Times,serif\" font-size=\"14.00\">Instruction (data: x)</text>\n",
       "</g>\n",
       "<!-- I&#45;&gt;S -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>I&#45;&gt;S</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M507.36,-75.8C464.22,-63.15 401.54,-44.75 359.29,-32.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"360.32,-28.72 349.74,-29.27 358.35,-35.44 360.32,-28.72\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x1bca5ad90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    from graphviz import Digraph\n",
    "    dot = Digraph()\n",
    "    dot.node('M', 'Model', style='filled',fillcolor=\"green\")\n",
    "    dot.node('B', 'Other Mediator', style='filled', fillcolor=\"red\",)\n",
    "    dot.node('L', 'Length of Output', style='filled', fillcolor=\"red\",)\n",
    "    dot.node('I', 'Instruction (data: x)', style='filled', fillcolor=\"lightgray\")\n",
    "    dot.node('S', 'Win-rate')\n",
    "    dot.edge('M', 'L')\n",
    "    dot.edge('M', 'B')\n",
    "    dot.edge('B', 'S')\n",
    "    dot.edge('L', 'S')\n",
    "    dot.edge('I', 'S')\n",
    "    dot.edge('M', 'S', \"  Desired interaction\", color=\"green\", fontcolor=\"green\")\n",
    "    dot.render('causal_graph', format='png', cleanup=True)\n",
    "    display(dot)\n",
    "except ImportError:\n",
    "    from IPython.display import Image\n",
    "    Image(filename=\"notebooks/causal_graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87764e-0aa9-4bfa-bf76-87e06e8c2080",
   "metadata": {},
   "source": [
    "The benefit of taking such perspective is that:\n",
    "1. it gives us a very general way of dealing with \"biases\"/mediators we do not want to consider (in our case, length, but this can be any mediator).\n",
    "2. this is well studied, and a very common approach to dealing with simialr problems to ours (e.g. in social sciences, clinical studies, econometrics, ...)\n",
    "\n",
    "So all we need is to write a reasonable (generalized) **linear structural equation** that conditions on:\n",
    "- undesirable mediators (red above) => condition on covariate you don't want to consider. We will later set those to the same as the baseline model.\n",
    "- on the treatment (green above) => measure the actual interaction you care about.\n",
    "- on any other covariate that does not interact with mediators & the treatment. This simply improves our estimation.\n",
    "\n",
    "A natural choice is the following:\n",
    "\n",
    "$$win\\_rate(m) = \\frac{1}{N} \\sum_{i=1}^{N} logistic(\\mathbf{w}_l[m] * binned(length(x_i)) + \\mathbf{w}_x^T \\cdot C(x_i) + \\mathbf{w}_m[m])$$\n",
    "\n",
    "Where words that start with an upper case letter are (or return) vectors, $C()$ one hot-encodes, $\\mathrm{W}_*$|$\\mathbf{w}_*$|$w_*$ are respectively is a matrix|vector|scalar of trainable weights, $^T \\cdot$ is dot product, $x$ is the instruction, $m$ is the model, and $binned$ bins into quantile.\n",
    "\n",
    "Note:\n",
    "- win-rate is the probability of beating the baseline => natural to predict with logistic fn\n",
    "- everything is a pretty natural choice besides $binned$, which makes the assumption that the length quantile has a linear effect on the (logit of the) output. I actually tried making fewer assumption by using $\\mathbf{w}_l[m]^T \\cdot C(binned(length(x)))$ but it didn't work as well.\n",
    "- very easy to add new mediators when we will want to control for those"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a99823-8197-44a1-8b69-48d2050ed569",
   "metadata": {},
   "source": [
    "- **What**: fit a logistic between instruction / length / model -> win_rate. This will be estimate to give the direct effect of the model on the win_rate, while conditioning away the effect of mediators. We could then use $\\mathbf{w}_m[m]$ as a length-controlled metric, but given that we want the metric to still be interpreted on the same scale as the previous win rate, we will instead set the mediators to the values from the baseline (i.e. same length of output). This can then essentially be thought of as \"what would be the win-rate if the model's outputs were the same length as the baseline ones\" (actual causal conclusions would need more care).\n",
    "- **Benefits**:\n",
    "    - **(D1-D3) metrics**: much better in all metrics than raw win-rate, even better than the two previous ones.\n",
    "    - standard: standard analysis for controlling covariates.\n",
    "    - **(D5) ~interpretability**: similar interpretation as win-rate and same scale. It's still harder to interpret than raw/balanced win-rate, but simpler than length normalized.\n",
    "    - **(D6) generality**: the procedure is extremely general, e.g., if we wanted to control for the amount of lists we could jsut throw that as a covariate. Furthermore, this is the ~likely the best things to do from a statistical point of view (assuming you don't have enough examples for propensity score matching) => giving the right example for other benchmarks.\n",
    "    - **(D7) robustness**: compared to the two previous examples this method is robust for whatever lengths of enw models, given that the parameters&binning will be fitted on the new models independently of the the previous ones.\n",
    "    - **(D8) ~independence**: the only parameters that need to be fitted across models is $\\mathbf{w}_x^T$, which can be thought of as \"the difficulty\" of the current example. Given that we already have 120 examples for each of those parameters, we can fit them nwo and hold those constants for new models. Conditioned on those parameter, we will be able to control the lengths for new models independently of other models => no need to recompute the rankings.\n",
    "- **Downside**:\n",
    "    - ~interpretability: although regression analysis for conrolling covaraites is common in stats, it might nto be as natural for some of the NLP / LLM-enthousiast community\n",
    "    - functional form: although pretty standard it's unclear whether the functional form (e.g. lienarity) really makes sense\n",
    "    - **(D4) simplicity**: the procedure is more involved than for previous metrics, which will make it harder for people to understand and might decrease the use of it. \n",
    "\n",
    "Overall, I think this is the best option we have, but I do worry about the fact that it makes the metric harder to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9d206b11-35ad-4113-8a64-cf2b100865fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "def make_data(all_df_annotations, instruction_difficulty=None, n_bins = 20):\n",
    "    df = all_df_annotations.copy()\n",
    "    df[\"delta_len\"] = df[\"len_1\"] - df[\"len_2\"]\n",
    "    df[\"rand_delta_len\"] = df[\"delta_len\"].astype(float) + ((np.random.rand(len(df))-0.5)*1e-3 )\n",
    "    df[f\"grouped_q{n_bins}_delta_len\"] = df.groupby(\"generator_2\")[\"rand_delta_len\"].transform(lambda x: pd.qcut(x, n_bins, labels=False, duplicates=\"drop\"))\n",
    "    df[\"instruction_difficulty\"] = df[\"index\"].transform(lambda g: instruction_difficulty[g])\n",
    "\n",
    "    rows_per_model = {}\n",
    "    sub_df = instruction_difficulty.to_frame().reset_index(drop=False).copy()\n",
    "    all_models = df[\"generator_2\"].unique()\n",
    "    ordered_models = ['gpt4_1106_preview'] + [m for m in all_models if m != \"gpt4_1106_preview\"]\n",
    "    for m in ordered_models:\n",
    "        rows_per_model[m] = sub_df.copy()\n",
    "        rows_per_model[m][\"generator_2\"] = m\n",
    "    df_lb = pd.concat(rows_per_model.values(), axis=0)\n",
    "    group_qn_no_delta = df.groupby(\"generator_2\").apply(lambda d: d[d[\"delta_len\"].abs() == d[\"delta_len\"].abs().min()][f\"grouped_q{n_bins}_delta_len\"].iloc[0], include_groups=False)\n",
    "    df_lb[f\"grouped_q{n_bins}_delta_len\"] = df_lb[\"generator_2\"].transform(lambda g: group_qn_no_delta[g])\n",
    "\n",
    "    return df, df_lb\n",
    "\n",
    "\n",
    "def logloss(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def logloss_sampleweight(y_true, y_pred, sample_weight):\n",
    "    # y_true is binary and sample_weight says the proba for that class. Let's revert that transform\n",
    "    y_true = np.where(y_true==1, sample_weight, 1-sample_weight)\n",
    "    return logloss(y_true, y_pred)\n",
    "\n",
    "\n",
    "def make_dmatrix_for_model(df, df_lb, formula=\"C(grouped_q10_delta_len) + instruction_difficulty \", col_y_true=\"preference\"):\n",
    "    df_inp = dmatrix(formula, df , return_type=\"dataframe\")\n",
    "    df_inp_lb = build_design_matrices([df_inp.design_info], df_lb, return_type='dataframe')[0]\n",
    "    df_inp[col_y_true] = df[col_y_true]\n",
    "    return df_inp, df_inp_lb\n",
    "    \n",
    "def fit_LogisticRegressionCV(data, col_y_true, is_ytrue_proba=True, n_splits=5, C=100, **kwargs):\n",
    "    \"\"\"Fits LogisticRegressionCV with optionally y_true being probabilities rather than the labels.\"\"\"\n",
    "    dflt_kwargs = dict(random_state=123, dual=False, penalty=\"l1\", solver='liblinear', n_jobs=None, fit_intercept=False)\n",
    "    dflt_kwargs.update(kwargs)\n",
    "    if not is_ytrue_proba:\n",
    "        if n_splits > 0:\n",
    "            model = LogisticRegressionCV(cv=n_splits, **dflt_kwargs)\n",
    "        else:\n",
    "            model = LogisticRegression(C=C, **dflt_kwargs)\n",
    "        model.fit(data.drop(columns=[col_y_true]), (data[col_y_true]).round().astype(int))\n",
    "    else:\n",
    "        # duplicate the df, once with label 0 and once with label 1\n",
    "        data = data.reset_index(drop=True).reset_index(drop=False, names=[\"group\"])\n",
    "        data_1 = data.copy()\n",
    "        data_1[\"y\"] = 1\n",
    "        data_0 = data.copy()\n",
    "        data_0[col_y_true] = 1 - data_0[col_y_true]\n",
    "        data_0[\"y\"] = 0\n",
    "        data_dup = pd.concat([data_1, data_0], axis=0).reset_index(drop=True)\n",
    "        if n_splits > 0:\n",
    "            cv = GroupKFold(n_splits=n_splits) \n",
    "            scorer = make_scorer(logloss_sampleweight, response_method=\"predict_proba\", greater_is_better=False).set_score_request(sample_weight=True)\n",
    "            model = LogisticRegressionCV(cv=cv, scoring=scorer, **dflt_kwargs)\n",
    "            fit_kwargs = dict(groups=data_dup[\"group\"])\n",
    "        else:\n",
    "            model = LogisticRegression(C=C, **dflt_kwargs)\n",
    "            fit_kwargs = dict()\n",
    "            \n",
    "        model.set_fit_request(sample_weight=True)\n",
    "        model.fit(X=data_dup.drop(columns=[col_y_true, \"y\", \"group\"]), \n",
    "                  y=data_dup[\"y\"], \n",
    "                  sample_weight=data_dup[col_y_true], **fit_kwargs)\n",
    "    return model\n",
    "\n",
    "def disjoint_optimization_(df, df_lb, formula):\n",
    "    all_reports = dict()\n",
    "    curr_df_lb = df_lb.copy()\n",
    "    for m in df[\"generator_2\"].unique():\n",
    "        # this is what will actually have to run for each new model\n",
    "        df_input, df_input_lb = make_dmatrix_for_model(df.query(f\"generator_2 == '{m}'\"), \n",
    "                                                       curr_df_lb.query(f\"generator_2 == '{m}'\"), \n",
    "                                                       formula=formula)\n",
    "        model = fit_LogisticRegressionCV(df_input, \"preference\", is_ytrue_proba=True, n_splits=5)\n",
    "        curr_df_lb.loc[curr_df_lb[\"generator_2\"] == m, \"preference\"] = model.predict_proba(df_input_lb)[:,1]\n",
    "    lb[formula] = curr_df_lb.groupby(\"generator_2\")[\"preference\"].mean()[lb.index] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33839356-e4cf-4e05-9e64-1584040adc23",
   "metadata": {},
   "source": [
    "First, let's load the instruction complexity ($\\mathbf{w}_x^T$) which I precomputed by joint fitting across all the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1768858-497f-490f-8be5-8d2fdf41f1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "out = hf_hub_download(repo_id=\"tatsu-lab/alpaca_eval\", \n",
    "                filename=\"instruction_difficulty.csv\",\n",
    "                repo_type=\"dataset\",\n",
    "                cache_dir=constants.DEFAULT_CACHE_DIR)\n",
    "               \n",
    "instruction_difficulty = pd.read_csv(out, index_col=0).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6b3f3460-ba69-4994-91c6-19f4931f334d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.5 s, sys: 82.7 ms, total: 9.59 s\n",
      "Wall time: 9.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "formula=f\"grouped_q20_delta_len + instruction_difficulty\"\n",
    "\n",
    "# df_lb contains the covariate that are held the same as the baseline \n",
    "df, df_lb = make_data(all_df_annotations, instruction_difficulty=instruction_difficulty)\n",
    "\n",
    "# run the disjoint optimization for all models.\n",
    "disjoint_optimization_(df, df_lb, formula=formula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f7e0236-9115-4c50-b214-82faa6be7c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report for **grouped_q20_delta_len + instruction_difficulty**\n",
      "\n",
      "## Gameability (lower is better)\n",
      "Verbosity gameability (relative std metric): 9.6%\n",
      "Conciseness gameability (relative std metric): 14.5%\n",
      "\n",
      "## Correlation with Arena (higher is better)\n",
      "Spearman Corr: 0.967\n",
      "Kendall Corr: 0.849\n",
      "\n",
      "## Correlation with length (closer to spearman=0.24, kendall=0.16 is better)\n",
      "Spearman Corr: 0.218\n",
      "Kendall Corr: 0.135\n",
      "\n",
      "## Top 20 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gpt4_1106_preview                        50.000000\n",
       "gpt4_1106_preview_verbose                48.410368\n",
       "gpt4_1106_preview_concise                41.483510\n",
       "Qwen1.5-72B-Chat                         40.423848\n",
       "gpt4                                     38.343078\n",
       "gpt4_0613_verbose                        35.900803\n",
       "gpt4_0314                                35.571320\n",
       "claude-2.1_verbose                       33.344125\n",
       "mistral-medium                           31.149406\n",
       "Snorkel-Mistral-PairRM-DPO-best-of-16    30.419699\n",
       "claude-2                                 29.698834\n",
       "claude                                   29.321304\n",
       "claude-2.1                               28.988286\n",
       "pairrm-Yi-34B-Chat                       28.871299\n",
       "gpt4_0613                                28.327658\n",
       "gemini-pro                               28.093753\n",
       "Snorkel-Mistral-PairRM-DPO               27.529751\n",
       "claude-instant-1.2                       27.454798\n",
       "Yi-34B-Chat                              26.488479\n",
       "xwinlm-70b-v0.1                          25.790748\n",
       "Name: grouped_q20_delta_len + instruction_difficulty, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Bottom 20 models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "guanaco-33b                     7.282495\n",
       "baize-v2-13b                    7.276935\n",
       "ultralm-13b                     7.252760\n",
       "vicuna-7b                       6.821999\n",
       "alpaca-7b                       6.679454\n",
       "alpaca-farm-ppo-sim-gpt4-20k    6.467621\n",
       "falcon-40b-instruct             6.466569\n",
       "falcon-7b-instruct              6.430307\n",
       "phi-2-sft                       6.184837\n",
       "minichat-3b                     6.181155\n",
       "chatglm2-6b                     5.898332\n",
       "alpaca-7b_concise               5.754277\n",
       "llama-2-7b-chat-hf              5.711359\n",
       "openbuddy-falcon-7b-v6          5.598822\n",
       "oasst-sft-pythia-12b            5.394793\n",
       "baize-v2-7b                     4.713379\n",
       "pythia-12b-mix-sft              4.275406\n",
       "guanaco-13b                     3.702956\n",
       "guanaco-7b                      3.520180\n",
       "baichuan-13b-chat               2.851103\n",
       "Name: grouped_q20_delta_len + instruction_difficulty, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "report(lb, formula, n_toshow=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf64f45-1d28-472b-9bad-056e344f93f9",
   "metadata": {},
   "source": [
    "We see that:\n",
    "- gameability is low (lower than the two other metrics)\n",
    "- length correlation is similar to Arena (similar to length normalized)\n",
    "- human correlation is extremely high (even higher than length normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05047e5b-283b-46e3-9bd4-bec32fc67ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
